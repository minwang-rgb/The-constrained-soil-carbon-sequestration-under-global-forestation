{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f585a1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble XGBoost Pipeline for Geospatial SOC Topsoil soil Uncertainty Quantification Under Current Climate\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import rioxarray as rxr\n",
    "import xarray as xr\n",
    "import rasterio\n",
    "from pathlib import Path\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "\n",
    "# ====== Configuration ======\n",
    "cv_model_paths = glob.glob('E:/minmin/cv_models/passive_top/*.json') # replace with 'active_top','SOC_top','passive_top'\n",
    "input_tif_folder = 'E:/cleaned_tifs_no_extremes_iqr'\n",
    "output_prediction_path = 'E:/minmin/passive_top_prediction_current.tif'# replace with 'active_top','SOC_top','passive_top'\n",
    "output_spread_path = 'E:/minmin/passive_top_uncertainty_spread_current.tif'# replace with 'active_top','SOC_top','passive_top'\n",
    "CHUNK_SIZE = 2000\n",
    "MAX_FEATURES_IN_MEMORY = 30 \n",
    "\n",
    "def check_memory_usage():\n",
    "    \"\"\"检查当前内存使用情况\"\"\"\n",
    "    mem = psutil.virtual_memory()\n",
    "    print(f\"  Memory: {mem.used/1e9:.1f}GB / {mem.total/1e9:.1f}GB ({mem.percent}%)\")\n",
    "    return mem.percent\n",
    "\n",
    "# ====== Load Models ======\n",
    "print(\"Loading models...\")\n",
    "cv_models = []\n",
    "for i, path in enumerate(cv_model_paths):\n",
    "    try:\n",
    "        model = xgb.Booster()\n",
    "        model.load_model(path)\n",
    "        cv_models.append(model)\n",
    "        print(f\"  Model {i+1}: Loaded from {os.path.basename(path)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Model {i+1}: Failed to load - {e}\")\n",
    "\n",
    "if not cv_models:\n",
    "    raise ValueError(\"No models loaded successfully!\")\n",
    "\n",
    "# Get feature names from first model\n",
    "model_feature_names = cv_models[0].feature_names\n",
    "print(f\"\\nModel feature names ({len(model_feature_names)}):\")\n",
    "for i in range(0, min(30, len(model_feature_names)), 10):\n",
    "    print(f\"  {model_feature_names[i:i+10]}\")\n",
    "\n",
    "# 分析特征类型\n",
    "base_features = []\n",
    "derived_features = []\n",
    "for feat in model_feature_names:\n",
    "    if '_log' in feat or '_interaction' in feat or '_squared' in feat or '_boost' in feat or '_bins' in feat:\n",
    "        derived_features.append(feat)\n",
    "    else:\n",
    "        base_features.append(feat)\n",
    "\n",
    "print(f\"\\nFeature analysis:\")\n",
    "print(f\"  Base features: {len(base_features)}\")\n",
    "print(f\"  Derived features: {len(derived_features)}\")\n",
    "\n",
    "# ====== 关键映射表 ======\n",
    "simple_tif_to_model = {\n",
    "    'Landuse_type': 'LUtype', 'LUtype': 'LUtype', 'LU_type': 'LUtype',\n",
    "    'Recovery_mode': 'Recovmode', 'Recovmode': 'Recovmode',\n",
    "    'BD': 't_bd', 't_bd': 't_bd',\n",
    "    'pH': 't_ph', 't_ph': 't_ph',\n",
    "    'Sand': 't_sand', 't_sand': 't_sand',\n",
    "    'Silt': 't_silt', 't_silt': 't_silt',\n",
    "    'Clay': 't_clay', 't_clay': 't_clay',\n",
    "    'Vege_type': 'Vegetype', 'Vegetype': 'Vegetype',\n",
    "    'TC': 't_oc', 't_oc': 't_oc',\n",
    "    'TN': 'TN13', 'TN13': 'TN13',\n",
    "    'TK': 'TK13', 'TK13': 'TK13',\n",
    "    'Altitude': 'Altitude', 'elevation': 'Altitude',\n",
    "    'ForestAge_TC000': 'Age', 'Age': 'Age',\n",
    "    'Lon': 'x', 'Lat': 'y',\n",
    "}\n",
    "\n",
    "# 反向映射\n",
    "model_to_tif = {}\n",
    "for tif_name, model_name in simple_tif_to_model.items():\n",
    "    if model_name not in model_to_tif:\n",
    "        model_to_tif[model_name] = []\n",
    "    model_to_tif[model_name].append(tif_name)\n",
    "\n",
    "# ====== 获取参考栅格信息 ======\n",
    "def get_reference_info(tif_folder):\n",
    "    \"\"\"获取参考栅格的详细信息\"\"\"\n",
    "    print(f\"\\n=== Getting reference raster info ===\")\n",
    "    \n",
    "    all_tif_files = list(Path(tif_folder).glob(\"*.tif\"))\n",
    "    if not all_tif_files:\n",
    "        raise ValueError(f\"No TIF files found in {tif_folder}\")\n",
    "    \n",
    "    # 使用Recovery_mode或Recovmode作为参考\n",
    "    ref_candidates = ['Recovery_mode', 'Recovmode', 'Landuse_type', 'Altitude']\n",
    "    ref_path = None\n",
    "    \n",
    "    for candidate in ref_candidates:\n",
    "        candidate_path = os.path.join(tif_folder, f\"{candidate}.tif\")\n",
    "        if os.path.exists(candidate_path):\n",
    "            ref_path = candidate_path\n",
    "            break\n",
    "    \n",
    "    if not ref_path:\n",
    "        ref_path = str(all_tif_files[0])\n",
    "    \n",
    "    print(f\"Using reference raster: {os.path.basename(ref_path)}\")\n",
    "    \n",
    "    with rasterio.open(ref_path) as src:\n",
    "        height, width = src.height, src.width\n",
    "        transform = src.transform\n",
    "        crs = src.crs\n",
    "        dtype = src.dtypes[0]\n",
    "    \n",
    "    # 计算总内存需求估计\n",
    "    total_pixels = height * width\n",
    "    bytes_per_pixel = 4  # float32\n",
    "    estimated_memory = len(base_features) * total_pixels * bytes_per_pixel / 1e9\n",
    "    \n",
    "    print(f\"  Shape: {height} x {width} = {total_pixels:,} pixels\")\n",
    "    print(f\"  Data type: {dtype}\")\n",
    "    print(f\"  CRS: {crs}\")\n",
    "    print(f\"  Estimated memory for base features: {estimated_memory:.2f} GB\")\n",
    "    print(f\"  Chunk size: {CHUNK_SIZE} rows\")\n",
    "    print(f\"  Pixels per chunk: {CHUNK_SIZE * width:,}\")\n",
    "    \n",
    "    return height, width, transform, crs\n",
    "\n",
    "# ====== 预加载特征元数据 ======\n",
    "def preload_feature_metadata(tif_folder, model_features):\n",
    "    \"\"\"预加载特征文件的路径和元数据\"\"\"\n",
    "    print(f\"\\n=== Preloading feature metadata ===\")\n",
    "    \n",
    "    all_tif_files = list(Path(tif_folder).glob(\"*.tif\"))\n",
    "    tif_names = [f.stem for f in all_tif_files]\n",
    "    \n",
    "    feature_metadata = {}\n",
    "    \n",
    "    for model_feat in base_features:  # 只需要基础特征\n",
    "        possible_tif_names = model_to_tif.get(model_feat, [model_feat])\n",
    "        tif_path = None\n",
    "        \n",
    "        for tif_name in possible_tif_names:\n",
    "            if tif_name in tif_names:\n",
    "                tif_path = os.path.join(tif_folder, f\"{tif_name}.tif\")\n",
    "                break\n",
    "        \n",
    "        if tif_path and os.path.exists(tif_path):\n",
    "            try:\n",
    "                with rasterio.open(tif_path) as src:\n",
    "                    # 只存储元数据，不加载数据\n",
    "                    feature_metadata[model_feat] = {\n",
    "                        'path': tif_path,\n",
    "                        'dtype': src.dtypes[0],\n",
    "                        'nodata': src.nodata\n",
    "                    }\n",
    "                    print(f\"  ✓ {model_feat} -> {os.path.basename(tif_path)}\")\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    print(f\"  Found metadata for {len(feature_metadata)} base features\")\n",
    "    return feature_metadata\n",
    "\n",
    "# ====== 高效加载分块数据 ======\n",
    "def load_chunk_data_efficient(feature_metadata, row_start, row_end, width, transform):\n",
    "    \"\"\"高效加载分块数据\"\"\"\n",
    "    chunk_data = {}\n",
    "    chunk_height = row_end - row_start\n",
    "    \n",
    "    if chunk_height <= 0:\n",
    "        return chunk_data\n",
    "    \n",
    "    # 加载基础特征\n",
    "    for model_feat, meta in feature_metadata.items():\n",
    "        try:\n",
    "            with rasterio.open(meta['path']) as src:\n",
    "                # 读取指定窗口\n",
    "                window = ((row_start, row_end), (0, width))\n",
    "                data = src.read(1, window=window)\n",
    "                \n",
    "                # 转换为float32，处理nodata\n",
    "                data = data.astype(np.float32)\n",
    "                if meta['nodata'] is not None:\n",
    "                    data[data == meta['nodata']] = np.nan\n",
    "                \n",
    "                chunk_data[model_feat] = data\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"    Error loading {model_feat}: {e}\")\n",
    "            # 创建空数组作为占位符\n",
    "            chunk_data[model_feat] = np.full((chunk_height, width), np.nan, dtype=np.float32)\n",
    "    \n",
    "    # 生成坐标特征（如果需要）\n",
    "    if 'x' in base_features:\n",
    "        x_coords = np.arange(width) * transform[0] + transform[2] + transform[0] / 2\n",
    "        chunk_data['x'] = np.tile(x_coords, (chunk_height, 1)).astype(np.float32)\n",
    "    \n",
    "    if 'y' in base_features:\n",
    "        y_coords = np.arange(row_start, row_end) * transform[4] + transform[5] + transform[4] / 2\n",
    "        chunk_data['y'] = np.tile(y_coords.reshape(-1, 1), (1, width)).astype(np.float32)\n",
    "    \n",
    "    return chunk_data\n",
    "\n",
    "# ====== 批量创建衍生特征 ======\n",
    "def create_derived_features_batch(chunk_data, model_features):\n",
    "    \"\"\"批量创建所有衍生特征\"\"\"\n",
    "    processed = chunk_data.copy()\n",
    "    \n",
    "    if not processed:\n",
    "        return processed\n",
    "    \n",
    "    chunk_rows, chunk_cols = next(iter(processed.values())).shape\n",
    "    \n",
    "    # 1. 对数变换\n",
    "    log_transforms = [\n",
    "        ('x', 'Lon_log'), ('y', 'Lat_log'), \n",
    "        ('Age', 'Age_log'), ('t_bd', 'BD_log'), ('t_ph', 'pH_log')\n",
    "    ]\n",
    "    \n",
    "    for base_feat, log_feat in log_transforms:\n",
    "        if log_feat in model_features and base_feat in processed:\n",
    "            data = processed[base_feat].copy()\n",
    "            mask = ~np.isnan(data)\n",
    "            if np.any(mask):\n",
    "                data[mask] = np.log(data[mask] + 1e-8)\n",
    "            processed[log_feat] = data\n",
    "    \n",
    "    # 2. LUtype相关特征\n",
    "    if 'LUtype' in processed:\n",
    "        lu_data = processed['LUtype'].copy()\n",
    "        lu_filled = np.where(np.isnan(lu_data), 0, lu_data)\n",
    "        \n",
    "        # LUtype增强特征\n",
    "        lu_boost_features = [f for f in model_features if f.startswith('LUtype_boost_')]\n",
    "        for boost_feat in lu_boost_features:\n",
    "            # 提取boost编号\n",
    "            try:\n",
    "                boost_num = int(boost_feat.split('_')[-1])\n",
    "                processed[boost_feat] = lu_filled * boost_num if boost_num > 1 else lu_filled\n",
    "            except:\n",
    "                processed[boost_feat] = lu_filled\n",
    "        \n",
    "        # LUtype平方\n",
    "        if 'LUtype_squared' in model_features:\n",
    "            processed['LUtype_squared'] = lu_filled ** 2\n",
    "        \n",
    "        # LUtype交互特征\n",
    "        for other_feat in ['t_ph', 't_bd', 'Age', 'Altitude']:\n",
    "            interaction_name = f'LUtype_{other_feat}_interaction'\n",
    "            if interaction_name in model_features and other_feat in processed:\n",
    "                other_data = processed[other_feat].copy()\n",
    "                other_filled = np.where(np.isnan(other_data), 0, other_data)\n",
    "                processed[interaction_name] = lu_filled * other_filled\n",
    "    \n",
    "    # 3. Altitude_bins\n",
    "    if 'Altitude_bins' in model_features and 'Altitude' in processed:\n",
    "        alt_data = processed['Altitude'].copy()\n",
    "        alt_filled = np.where(np.isnan(alt_data), 0, alt_data)\n",
    "        processed['Altitude_bins'] = alt_filled\n",
    "    \n",
    "    # 4. 填充缺失特征\n",
    "    missing = [f for f in model_features if f not in processed]\n",
    "    if missing:\n",
    "        for feat in missing:\n",
    "            processed[feat] = np.zeros((chunk_rows, chunk_cols), dtype=np.float32)\n",
    "    \n",
    "    return processed\n",
    "\n",
    "# ====== 批量预测 ======\n",
    "def predict_chunk_batch(chunk_data, models):\n",
    "    \"\"\"对分块数据进行批量预测\"\"\"\n",
    "    if not models or not chunk_data:\n",
    "        return None, None\n",
    "    \n",
    "    model_features = models[0].feature_names\n",
    "    \n",
    "    # 创建衍生特征\n",
    "    all_features = create_derived_features_batch(chunk_data, model_features)\n",
    "    \n",
    "    # 获取分块形状\n",
    "    chunk_rows, chunk_cols = next(iter(all_features.values())).shape\n",
    "    \n",
    "    # 使用Recovmode或第一个非坐标特征作为有效掩膜\n",
    "    core_features = ['Recovmode', 'Recovery_mode', 'LUtype', 'Altitude']\n",
    "    core_feature = None\n",
    "    for cf in core_features:\n",
    "        if cf in all_features:\n",
    "            core_feature = cf\n",
    "            break\n",
    "    \n",
    "    if core_feature:\n",
    "        valid_mask = ~np.isnan(all_features[core_feature])\n",
    "    else:\n",
    "        valid_mask = np.ones((chunk_rows, chunk_cols), dtype=bool)\n",
    "    \n",
    "    # 准备特征数组\n",
    "    feature_arrays = []\n",
    "    for feat in model_features:\n",
    "        if feat in all_features:\n",
    "            feature_arrays.append(all_features[feat])\n",
    "        else:\n",
    "            feature_arrays.append(np.zeros((chunk_rows, chunk_cols), dtype=np.float32))\n",
    "    \n",
    "    # 转换为2D数组\n",
    "    X_3d = np.stack(feature_arrays, axis=-1)\n",
    "    X_flat = X_3d.reshape(-1, len(model_features))\n",
    "    \n",
    "    # 提取有效像素\n",
    "    valid_indices = valid_mask.flatten()\n",
    "    X_valid = X_flat[valid_indices]\n",
    "    \n",
    "    if len(X_valid) == 0:\n",
    "        return None, None\n",
    "    \n",
    "    # 批量预测（使用所有模型）\n",
    "    all_predictions = []\n",
    "    for model in models:\n",
    "        dmatrix = xgb.DMatrix(X_valid, feature_names=model_features)\n",
    "        preds = model.predict(dmatrix)\n",
    "        all_predictions.append(preds)\n",
    "    \n",
    "    # 计算均值和标准差\n",
    "    preds_array = np.array(all_predictions)\n",
    "    mean_pred = np.mean(preds_array, axis=0)\n",
    "    std_pred = np.std(preds_array, axis=0)\n",
    "    \n",
    "    # 重建分块\n",
    "    mean_full = np.full(chunk_rows * chunk_cols, np.nan, dtype=np.float32)\n",
    "    std_full = np.full(chunk_rows * chunk_cols, np.nan, dtype=np.float32)\n",
    "    \n",
    "    mean_full[valid_indices] = mean_pred\n",
    "    std_full[valid_indices] = std_pred\n",
    "    \n",
    "    return mean_full.reshape(chunk_rows, chunk_cols), std_full.reshape(chunk_rows, chunk_cols)\n",
    "\n",
    "# ====== 主程序 ======\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 70)\n",
    "    print(\"SOC Active Layer Prediction - Optimized for Large Memory Systems\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # 初始内存检查\n",
    "    print(f\"\\nInitial system check:\")\n",
    "    check_memory_usage()\n",
    "    \n",
    "    # 步骤1: 加载模型\n",
    "    print(f\"\\n1. Loading {len(cv_model_paths)} models...\")\n",
    "    \n",
    "    # 步骤2: 获取参考信息\n",
    "    print(f\"\\n2. Analyzing input data...\")\n",
    "    height, width, transform, crs = get_reference_info(input_tif_folder)\n",
    "    \n",
    "    # 步骤3: 预加载特征元数据\n",
    "    print(f\"\\n3. Preloading feature metadata...\")\n",
    "    feature_metadata = preload_feature_metadata(input_tif_folder, model_feature_names)\n",
    "    \n",
    "    if not feature_metadata:\n",
    "        raise ValueError(\"No base features found! Check your TIF files.\")\n",
    "    \n",
    "    # 步骤4: 分块处理\n",
    "    print(f\"\\n4. Starting chunked prediction...\")\n",
    "    \n",
    "    # 准备输出数组\n",
    "    mean_result = np.full((height, width), np.nan, dtype=np.float32)\n",
    "    std_result = np.full((height, width), np.nan, dtype=np.float32)\n",
    "    \n",
    "    total_chunks = int(np.ceil(height / CHUNK_SIZE))\n",
    "    total_valid_pixels = 0\n",
    "    \n",
    "    # 使用进度条\n",
    "    print(f\"\\nProgress:\")\n",
    "    \n",
    "    for chunk_idx in range(total_chunks):\n",
    "        chunk_start = chunk_idx * CHUNK_SIZE\n",
    "        chunk_end = min(chunk_start + CHUNK_SIZE, height)\n",
    "        chunk_height = chunk_end - chunk_start\n",
    "        \n",
    "        print(f\"\\n  Chunk {chunk_idx+1}/{total_chunks}: Rows {chunk_start:,}-{chunk_end:,}\")\n",
    "        check_memory_usage()\n",
    "        \n",
    "        # 加载分块数据\n",
    "        chunk_data = load_chunk_data_efficient(\n",
    "            feature_metadata, chunk_start, chunk_end, width, transform\n",
    "        )\n",
    "        \n",
    "        if not chunk_data:\n",
    "            print(f\"    No data loaded, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"    Loaded {len(chunk_data)} features for this chunk\")\n",
    "        \n",
    "        # 预测\n",
    "        mean_chunk, std_chunk = predict_chunk_batch(chunk_data, cv_models)\n",
    "        \n",
    "        if mean_chunk is not None:\n",
    "            # 存储结果\n",
    "            mean_result[chunk_start:chunk_end, :] = mean_chunk\n",
    "            std_result[chunk_start:chunk_end, :] = std_chunk\n",
    "            \n",
    "            # 统计\n",
    "            valid_in_chunk = np.sum(~np.isnan(mean_chunk))\n",
    "            total_valid_pixels += valid_in_chunk\n",
    "            \n",
    "            chunk_min = np.nanmin(mean_chunk)\n",
    "            chunk_max = np.nanmax(mean_chunk)\n",
    "            print(f\"    Predicted: {valid_in_chunk:,} pixels\")\n",
    "            print(f\"    Range: [{chunk_min:.3f}, {chunk_max:.3f}]\")\n",
    "            print(f\"    Progress: {total_valid_pixels/(height*width)*100:.1f}%\")\n",
    "        \n",
    "        # 清理内存\n",
    "        del chunk_data, mean_chunk, std_chunk\n",
    "        gc.collect()\n",
    "    \n",
    "    # 步骤5: 保存结果\n",
    "    print(f\"\\n5. Saving results...\")\n",
    "    \n",
    "    try:\n",
    "        # 创建坐标\n",
    "        x_coords = np.arange(width) * transform[0] + transform[2] + transform[0] / 2\n",
    "        y_coords = np.arange(height) * transform[4] + transform[5] + transform[4] / 2\n",
    "        \n",
    "        # 保存预测均值\n",
    "        print(f\"  Saving prediction to: {output_prediction_path}\")\n",
    "        da_mean = xr.DataArray(\n",
    "            mean_result,\n",
    "            coords=[('y', y_coords), ('x', x_coords)],\n",
    "            dims=('y', 'x'),\n",
    "            name='SOC_prediction'\n",
    "        )\n",
    "        da_mean.rio.write_crs(crs, inplace=True)\n",
    "        da_mean.rio.write_transform(transform, inplace=True)\n",
    "        \n",
    "        da_mean.rio.to_raster(\n",
    "            output_prediction_path,\n",
    "            driver='GTiff',\n",
    "            dtype=np.float32,\n",
    "            compress='LZW',\n",
    "            nodata=np.nan,\n",
    "            tiled=True,\n",
    "            blockxsize=256,\n",
    "            blockysize=256,\n",
    "            BIGTIFF='IF_SAFER'  # 支持大文件\n",
    "        )\n",
    "        print(f\"  ✓ Prediction saved successfully\")\n",
    "        \n",
    "        # 保存不确定性\n",
    "        print(f\"  Saving uncertainty to: {output_spread_path}\")\n",
    "        da_std = xr.DataArray(\n",
    "            std_result,\n",
    "            coords=[('y', y_coords), ('x', x_coords)],\n",
    "            dims=('y', 'x'),\n",
    "            name='SOC_uncertainty'\n",
    "        )\n",
    "        da_std.rio.write_crs(crs, inplace=True)\n",
    "        da_std.rio.write_transform(transform, inplace=True)\n",
    "        \n",
    "        da_std.rio.to_raster(\n",
    "            output_spread_path,\n",
    "            driver='GTiff',\n",
    "            dtype=np.float32,\n",
    "            compress='LZW',\n",
    "            nodata=np.nan,\n",
    "            tiled=True,\n",
    "            blockxsize=256,\n",
    "            blockysize=256,\n",
    "            BIGTIFF='IF_SAFER'\n",
    "        )\n",
    "        print(f\"  ✓ Uncertainty saved successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error saving results: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # 最终统计\n",
    "    print(f\"\\n6. Final statistics:\")\n",
    "    total_pixels = height * width\n",
    "    \n",
    "    print(f\"  Total pixels: {total_pixels:,}\")\n",
    "    print(f\"  Predicted pixels: {total_valid_pixels:,} ({total_valid_pixels/total_pixels*100:.1f}%)\")\n",
    "    print(f\"  Missing pixels: {total_pixels-total_valid_pixels:,} ({(total_pixels-total_valid_pixels)/total_pixels*100:.1f}%)\")\n",
    "    \n",
    "    if total_valid_pixels > 0:\n",
    "        final_min = np.nanmin(mean_result)\n",
    "        final_max = np.nanmax(mean_result)\n",
    "        final_mean = np.nanmean(mean_result)\n",
    "        uncert_mean = np.nanmean(std_result)\n",
    "        \n",
    "        print(f\"  Prediction range: [{final_min:.4f}, {final_max:.4f}]\")\n",
    "        print(f\"  Prediction mean: {final_mean:.4f}\")\n",
    "        print(f\"  Uncertainty mean: {uncert_mean:.4f}\")\n",
    "        print(f\"  Uncertainty range: [{np.nanmin(std_result):.4f}, {np.nanmax(std_result):.4f}]\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 70)\n",
    "    print(\"✓ Analysis completed successfully!\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # 最终内存检查\n",
    "    print(f\"\\nFinal memory usage:\")\n",
    "    check_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6fa74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble XGBoost Pipeline for Geospatial SOC Subsoil Uncertainty Quantification Under Current Climate\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import rioxarray as rxr\n",
    "import xarray as xr\n",
    "import rasterio\n",
    "from pathlib import Path\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "\n",
    "# ====== Configuration ======\n",
    "cv_model_paths = glob.glob('E:/minmin/cv_models/passive_sub/*.json') # replace with 'active_sub','SOC_sub','passive_sub'\n",
    "input_tif_folder = 'E:/cleaned_tifs_no_extremes_iqr'\n",
    "output_prediction_path = 'E:/minmin/passive_sub_prediction_current.tif'# replace with 'active_sub','SOC_sub','passive_sub'\n",
    "output_spread_path = 'E:/minmin/passive_sub_uncertainty_spread_current.tif'# replace with 'active_sub','SOC_sub','passive_sub'\n",
    "CHUNK_SIZE = 2000\n",
    "MAX_FEATURES_IN_MEMORY = 30 \n",
    "\n",
    "# ====== 内存监控 ======\n",
    "def check_memory_usage():\n",
    "    mem = psutil.virtual_memory()\n",
    "    print(f\"  Memory: {mem.used/1e9:.1f}GB / {mem.total/1e9:.1f}GB ({mem.percent}%)\")\n",
    "    return mem.percent\n",
    "\n",
    "# ====== Load Models ======\n",
    "print(\"Loading models...\")\n",
    "cv_models = []\n",
    "for i, path in enumerate(cv_model_paths):\n",
    "    try:\n",
    "        model = xgb.Booster()\n",
    "        model.load_model(path)\n",
    "        cv_models.append(model)\n",
    "        print(f\"  Model {i+1}: Loaded from {os.path.basename(path)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Model {i+1}: Failed to load - {e}\")\n",
    "\n",
    "if not cv_models:\n",
    "    raise ValueError(\"No models loaded successfully!\")\n",
    "\n",
    "# Get feature names from first model\n",
    "model_feature_names = cv_models[0].feature_names\n",
    "print(f\"\\nModel feature names ({len(model_feature_names)}):\")\n",
    "for i in range(0, min(30, len(model_feature_names)), 10):\n",
    "    print(f\"  {model_feature_names[i:i+10]}\")\n",
    "\n",
    "# 分析特征类型\n",
    "base_features = []\n",
    "derived_features = []\n",
    "for feat in model_feature_names:\n",
    "    if '_log' in feat or '_interaction' in feat or '_squared' in feat or '_boost' in feat or '_bins' in feat:\n",
    "        derived_features.append(feat)\n",
    "    else:\n",
    "        base_features.append(feat)\n",
    "\n",
    "print(f\"\\nFeature analysis:\")\n",
    "print(f\"  Base features: {len(base_features)}\")\n",
    "print(f\"  Derived features: {len(derived_features)}\")\n",
    "\n",
    "# ====== 关键映射表 ======\n",
    "simple_tif_to_model = {\n",
    "    'Landuse_type': 'LUtype', 'LUtype': 'LUtype', 'LU_type': 'LUtype',\n",
    "    'Recovery_mode': 'Recovmode', 'Recovmode': 'Recovmode',\n",
    "    'BD': 's_bd', 's_bd': 's_bd',\n",
    "    'pH': 's_ph', 's_ph': 's_ph',\n",
    "    'Sand': 's_sand', 's_sand': 's_sand',\n",
    "    'Silt': 's_silt', 's_silt': 's_silt',\n",
    "    'Clay': 's_clay', 's_clay': 's_clay',\n",
    "    'Vege_type': 'Vegetype', 'Vegetype': 'Vegetype',\n",
    "    'TC': 's_oc', 's_oc': 's_oc',\n",
    "    'TN': 'TN46', 'TN46': 'TN46',\n",
    "    'TK': 'TK46', 'TK46': 'TK46',\n",
    "    'Altitude': 'Altitude', 'elevation': 'Altitude',\n",
    "    'ForestAge_TC000': 'Age', 'Age': 'Age',\n",
    "    'Lon': 'x', 'Lat': 'y',\n",
    "}\n",
    "\n",
    "# 反向映射\n",
    "model_to_tif = {}\n",
    "for tif_name, model_name in simple_tif_to_model.items():\n",
    "    if model_name not in model_to_tif:\n",
    "        model_to_tif[model_name] = []\n",
    "    model_to_tif[model_name].append(tif_name)\n",
    "\n",
    "# ====== 获取参考栅格信息 ======\n",
    "def get_reference_info(tif_folder):\n",
    "    \"\"\"获取参考栅格的详细信息\"\"\"\n",
    "    print(f\"\\n=== Getting reference raster info ===\")\n",
    "    \n",
    "    all_tif_files = list(Path(tif_folder).glob(\"*.tif\"))\n",
    "    if not all_tif_files:\n",
    "        raise ValueError(f\"No TIF files found in {tif_folder}\")\n",
    "    \n",
    "    # 使用Recovery_mode或Recovmode作为参考\n",
    "    ref_candidates = ['Recovery_mode', 'Recovmode', 'Landuse_type', 'Altitude']\n",
    "    ref_path = None\n",
    "    \n",
    "    for candidate in ref_candidates:\n",
    "        candidate_path = os.path.join(tif_folder, f\"{candidate}.tif\")\n",
    "        if os.path.exists(candidate_path):\n",
    "            ref_path = candidate_path\n",
    "            break\n",
    "    \n",
    "    if not ref_path:\n",
    "        ref_path = str(all_tif_files[0])\n",
    "    \n",
    "    print(f\"Using reference raster: {os.path.basename(ref_path)}\")\n",
    "    \n",
    "    with rasterio.open(ref_path) as src:\n",
    "        height, width = src.height, src.width\n",
    "        transform = src.transform\n",
    "        crs = src.crs\n",
    "        dtype = src.dtypes[0]\n",
    "    \n",
    "    # 计算总内存需求估计\n",
    "    total_pixels = height * width\n",
    "    bytes_per_pixel = 4  # float32\n",
    "    estimated_memory = len(base_features) * total_pixels * bytes_per_pixel / 1e9\n",
    "    \n",
    "    print(f\"  Shape: {height} x {width} = {total_pixels:,} pixels\")\n",
    "    print(f\"  Data type: {dtype}\")\n",
    "    print(f\"  CRS: {crs}\")\n",
    "    print(f\"  Estimated memory for base features: {estimated_memory:.2f} GB\")\n",
    "    print(f\"  Chunk size: {CHUNK_SIZE} rows\")\n",
    "    print(f\"  Pixels per chunk: {CHUNK_SIZE * width:,}\")\n",
    "    \n",
    "    return height, width, transform, crs\n",
    "\n",
    "# ====== 预加载特征元数据 ======\n",
    "def preload_feature_metadata(tif_folder, model_features):\n",
    "    \"\"\"预加载特征文件的路径和元数据\"\"\"\n",
    "    print(f\"\\n=== Preloading feature metadata ===\")\n",
    "    \n",
    "    all_tif_files = list(Path(tif_folder).glob(\"*.tif\"))\n",
    "    tif_names = [f.stem for f in all_tif_files]\n",
    "    \n",
    "    feature_metadata = {}\n",
    "    \n",
    "    for model_feat in base_features:  # 只需要基础特征\n",
    "        possible_tif_names = model_to_tif.get(model_feat, [model_feat])\n",
    "        tif_path = None\n",
    "        \n",
    "        for tif_name in possible_tif_names:\n",
    "            if tif_name in tif_names:\n",
    "                tif_path = os.path.join(tif_folder, f\"{tif_name}.tif\")\n",
    "                break\n",
    "        \n",
    "        if tif_path and os.path.exists(tif_path):\n",
    "            try:\n",
    "                with rasterio.open(tif_path) as src:\n",
    "                    # 只存储元数据，不加载数据\n",
    "                    feature_metadata[model_feat] = {\n",
    "                        'path': tif_path,\n",
    "                        'dtype': src.dtypes[0],\n",
    "                        'nodata': src.nodata\n",
    "                    }\n",
    "                    print(f\"  ✓ {model_feat} -> {os.path.basename(tif_path)}\")\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    print(f\"  Found metadata for {len(feature_metadata)} base features\")\n",
    "    return feature_metadata\n",
    "\n",
    "# ====== 高效加载分块数据 ======\n",
    "def load_chunk_data_efficient(feature_metadata, row_start, row_end, width, transform):\n",
    "    \"\"\"高效加载分块数据\"\"\"\n",
    "    chunk_data = {}\n",
    "    chunk_height = row_end - row_start\n",
    "    \n",
    "    if chunk_height <= 0:\n",
    "        return chunk_data\n",
    "    \n",
    "    # 加载基础特征\n",
    "    for model_feat, meta in feature_metadata.items():\n",
    "        try:\n",
    "            with rasterio.open(meta['path']) as src:\n",
    "                # 读取指定窗口\n",
    "                window = ((row_start, row_end), (0, width))\n",
    "                data = src.read(1, window=window)\n",
    "                \n",
    "                # 转换为float32，处理nodata\n",
    "                data = data.astype(np.float32)\n",
    "                if meta['nodata'] is not None:\n",
    "                    data[data == meta['nodata']] = np.nan\n",
    "                \n",
    "                chunk_data[model_feat] = data\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"    Error loading {model_feat}: {e}\")\n",
    "            # 创建空数组作为占位符\n",
    "            chunk_data[model_feat] = np.full((chunk_height, width), np.nan, dtype=np.float32)\n",
    "    \n",
    "    # 生成坐标特征（如果需要）\n",
    "    if 'x' in base_features:\n",
    "        x_coords = np.arange(width) * transform[0] + transform[2] + transform[0] / 2\n",
    "        chunk_data['x'] = np.tile(x_coords, (chunk_height, 1)).astype(np.float32)\n",
    "    \n",
    "    if 'y' in base_features:\n",
    "        y_coords = np.arange(row_start, row_end) * transform[4] + transform[5] + transform[4] / 2\n",
    "        chunk_data['y'] = np.tile(y_coords.reshape(-1, 1), (1, width)).astype(np.float32)\n",
    "    \n",
    "    return chunk_data\n",
    "\n",
    "# ====== 批量创建衍生特征 ======\n",
    "def create_derived_features_batch(chunk_data, model_features):\n",
    "    \"\"\"批量创建所有衍生特征\"\"\"\n",
    "    processed = chunk_data.copy()\n",
    "    \n",
    "    if not processed:\n",
    "        return processed\n",
    "    \n",
    "    chunk_rows, chunk_cols = next(iter(processed.values())).shape\n",
    "    \n",
    "    # 1. 对数变换\n",
    "    log_transforms = [\n",
    "        ('x', 'Lon_log'), ('y', 'Lat_log'), \n",
    "        ('Age', 'Age_log'), ('t_bd', 'BD_log'), ('t_ph', 'pH_log')\n",
    "    ]\n",
    "    \n",
    "    for base_feat, log_feat in log_transforms:\n",
    "        if log_feat in model_features and base_feat in processed:\n",
    "            data = processed[base_feat].copy()\n",
    "            mask = ~np.isnan(data)\n",
    "            if np.any(mask):\n",
    "                data[mask] = np.log(data[mask] + 1e-8)\n",
    "            processed[log_feat] = data\n",
    "    \n",
    "    # 2. LUtype相关特征\n",
    "    if 'LUtype' in processed:\n",
    "        lu_data = processed['LUtype'].copy()\n",
    "        lu_filled = np.where(np.isnan(lu_data), 0, lu_data)\n",
    "        \n",
    "        # LUtype增强特征\n",
    "        lu_boost_features = [f for f in model_features if f.startswith('LUtype_boost_')]\n",
    "        for boost_feat in lu_boost_features:\n",
    "            # 提取boost编号\n",
    "            try:\n",
    "                boost_num = int(boost_feat.split('_')[-1])\n",
    "                processed[boost_feat] = lu_filled * boost_num if boost_num > 1 else lu_filled\n",
    "            except:\n",
    "                processed[boost_feat] = lu_filled\n",
    "        \n",
    "        # LUtype平方\n",
    "        if 'LUtype_squared' in model_features:\n",
    "            processed['LUtype_squared'] = lu_filled ** 2\n",
    "        \n",
    "        # LUtype交互特征\n",
    "        for other_feat in ['t_ph', 't_bd', 'Age', 'Altitude']:\n",
    "            interaction_name = f'LUtype_{other_feat}_interaction'\n",
    "            if interaction_name in model_features and other_feat in processed:\n",
    "                other_data = processed[other_feat].copy()\n",
    "                other_filled = np.where(np.isnan(other_data), 0, other_data)\n",
    "                processed[interaction_name] = lu_filled * other_filled\n",
    "    \n",
    "    # 3. Altitude_bins\n",
    "    if 'Altitude_bins' in model_features and 'Altitude' in processed:\n",
    "        alt_data = processed['Altitude'].copy()\n",
    "        alt_filled = np.where(np.isnan(alt_data), 0, alt_data)\n",
    "        processed['Altitude_bins'] = alt_filled\n",
    "    \n",
    "    # 4. 填充缺失特征\n",
    "    missing = [f for f in model_features if f not in processed]\n",
    "    if missing:\n",
    "        for feat in missing:\n",
    "            processed[feat] = np.zeros((chunk_rows, chunk_cols), dtype=np.float32)\n",
    "    \n",
    "    return processed\n",
    "\n",
    "# ====== 批量预测 ======\n",
    "def predict_chunk_batch(chunk_data, models):\n",
    "    \"\"\"对分块数据进行批量预测\"\"\"\n",
    "    if not models or not chunk_data:\n",
    "        return None, None\n",
    "    \n",
    "    model_features = models[0].feature_names\n",
    "    \n",
    "    # 创建衍生特征\n",
    "    all_features = create_derived_features_batch(chunk_data, model_features)\n",
    "    \n",
    "    # 获取分块形状\n",
    "    chunk_rows, chunk_cols = next(iter(all_features.values())).shape\n",
    "    \n",
    "    # 使用Recovmode或第一个非坐标特征作为有效掩膜\n",
    "    core_features = ['Recovmode', 'Recovery_mode', 'LUtype', 'Altitude']\n",
    "    core_feature = None\n",
    "    for cf in core_features:\n",
    "        if cf in all_features:\n",
    "            core_feature = cf\n",
    "            break\n",
    "    \n",
    "    if core_feature:\n",
    "        valid_mask = ~np.isnan(all_features[core_feature])\n",
    "    else:\n",
    "        valid_mask = np.ones((chunk_rows, chunk_cols), dtype=bool)\n",
    "    \n",
    "    # 准备特征数组\n",
    "    feature_arrays = []\n",
    "    for feat in model_features:\n",
    "        if feat in all_features:\n",
    "            feature_arrays.append(all_features[feat])\n",
    "        else:\n",
    "            feature_arrays.append(np.zeros((chunk_rows, chunk_cols), dtype=np.float32))\n",
    "    \n",
    "    # 转换为2D数组\n",
    "    X_3d = np.stack(feature_arrays, axis=-1)\n",
    "    X_flat = X_3d.reshape(-1, len(model_features))\n",
    "    \n",
    "    # 提取有效像素\n",
    "    valid_indices = valid_mask.flatten()\n",
    "    X_valid = X_flat[valid_indices]\n",
    "    \n",
    "    if len(X_valid) == 0:\n",
    "        return None, None\n",
    "    \n",
    "    # 批量预测（使用所有模型）\n",
    "    all_predictions = []\n",
    "    for model in models:\n",
    "        dmatrix = xgb.DMatrix(X_valid, feature_names=model_features)\n",
    "        preds = model.predict(dmatrix)\n",
    "        all_predictions.append(preds)\n",
    "    \n",
    "    # 计算均值和标准差\n",
    "    preds_array = np.array(all_predictions)\n",
    "    mean_pred = np.mean(preds_array, axis=0)\n",
    "    std_pred = np.std(preds_array, axis=0)\n",
    "    \n",
    "    # 重建分块\n",
    "    mean_full = np.full(chunk_rows * chunk_cols, np.nan, dtype=np.float32)\n",
    "    std_full = np.full(chunk_rows * chunk_cols, np.nan, dtype=np.float32)\n",
    "    \n",
    "    mean_full[valid_indices] = mean_pred\n",
    "    std_full[valid_indices] = std_pred\n",
    "    \n",
    "    return mean_full.reshape(chunk_rows, chunk_cols), std_full.reshape(chunk_rows, chunk_cols)\n",
    "\n",
    "# ====== 主程序 ======\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 70)\n",
    "    print(\"SOC Active Layer Prediction - Optimized for Large Memory Systems\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # 初始内存检查\n",
    "    print(f\"\\nInitial system check:\")\n",
    "    check_memory_usage()\n",
    "    \n",
    "    # 步骤1: 加载模型\n",
    "    print(f\"\\n1. Loading {len(cv_model_paths)} models...\")\n",
    "    \n",
    "    # 步骤2: 获取参考信息\n",
    "    print(f\"\\n2. Analyzing input data...\")\n",
    "    height, width, transform, crs = get_reference_info(input_tif_folder)\n",
    "    \n",
    "    # 步骤3: 预加载特征元数据\n",
    "    print(f\"\\n3. Preloading feature metadata...\")\n",
    "    feature_metadata = preload_feature_metadata(input_tif_folder, model_feature_names)\n",
    "    \n",
    "    if not feature_metadata:\n",
    "        raise ValueError(\"No base features found! Check your TIF files.\")\n",
    "    \n",
    "    # 步骤4: 分块处理\n",
    "    print(f\"\\n4. Starting chunked prediction...\")\n",
    "    \n",
    "    # 准备输出数组\n",
    "    mean_result = np.full((height, width), np.nan, dtype=np.float32)\n",
    "    std_result = np.full((height, width), np.nan, dtype=np.float32)\n",
    "    \n",
    "    total_chunks = int(np.ceil(height / CHUNK_SIZE))\n",
    "    total_valid_pixels = 0\n",
    "    \n",
    "    # 使用进度条\n",
    "    print(f\"\\nProgress:\")\n",
    "    \n",
    "    for chunk_idx in range(total_chunks):\n",
    "        chunk_start = chunk_idx * CHUNK_SIZE\n",
    "        chunk_end = min(chunk_start + CHUNK_SIZE, height)\n",
    "        chunk_height = chunk_end - chunk_start\n",
    "        \n",
    "        print(f\"\\n  Chunk {chunk_idx+1}/{total_chunks}: Rows {chunk_start:,}-{chunk_end:,}\")\n",
    "        check_memory_usage()\n",
    "        \n",
    "        # 加载分块数据\n",
    "        chunk_data = load_chunk_data_efficient(\n",
    "            feature_metadata, chunk_start, chunk_end, width, transform\n",
    "        )\n",
    "        \n",
    "        if not chunk_data:\n",
    "            print(f\"    No data loaded, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"    Loaded {len(chunk_data)} features for this chunk\")\n",
    "        \n",
    "        # 预测\n",
    "        mean_chunk, std_chunk = predict_chunk_batch(chunk_data, cv_models)\n",
    "        \n",
    "        if mean_chunk is not None:\n",
    "            # 存储结果\n",
    "            mean_result[chunk_start:chunk_end, :] = mean_chunk\n",
    "            std_result[chunk_start:chunk_end, :] = std_chunk\n",
    "            \n",
    "            # 统计\n",
    "            valid_in_chunk = np.sum(~np.isnan(mean_chunk))\n",
    "            total_valid_pixels += valid_in_chunk\n",
    "            \n",
    "            chunk_min = np.nanmin(mean_chunk)\n",
    "            chunk_max = np.nanmax(mean_chunk)\n",
    "            print(f\"    Predicted: {valid_in_chunk:,} pixels\")\n",
    "            print(f\"    Range: [{chunk_min:.3f}, {chunk_max:.3f}]\")\n",
    "            print(f\"    Progress: {total_valid_pixels/(height*width)*100:.1f}%\")\n",
    "        \n",
    "        # 清理内存\n",
    "        del chunk_data, mean_chunk, std_chunk\n",
    "        gc.collect()\n",
    "    \n",
    "    # 步骤5: 保存结果\n",
    "    print(f\"\\n5. Saving results...\")\n",
    "    \n",
    "    try:\n",
    "        # 创建坐标\n",
    "        x_coords = np.arange(width) * transform[0] + transform[2] + transform[0] / 2\n",
    "        y_coords = np.arange(height) * transform[4] + transform[5] + transform[4] / 2\n",
    "        \n",
    "        # 保存预测均值\n",
    "        print(f\"  Saving prediction to: {output_prediction_path}\")\n",
    "        da_mean = xr.DataArray(\n",
    "            mean_result,\n",
    "            coords=[('y', y_coords), ('x', x_coords)],\n",
    "            dims=('y', 'x'),\n",
    "            name='SOC_prediction'\n",
    "        )\n",
    "        da_mean.rio.write_crs(crs, inplace=True)\n",
    "        da_mean.rio.write_transform(transform, inplace=True)\n",
    "        \n",
    "        da_mean.rio.to_raster(\n",
    "            output_prediction_path,\n",
    "            driver='GTiff',\n",
    "            dtype=np.float32,\n",
    "            compress='LZW',\n",
    "            nodata=np.nan,\n",
    "            tiled=True,\n",
    "            blockxsize=256,\n",
    "            blockysize=256,\n",
    "            BIGTIFF='IF_SAFER'  # 支持大文件\n",
    "        )\n",
    "        print(f\"  ✓ Prediction saved successfully\")\n",
    "        \n",
    "        # 保存不确定性\n",
    "        print(f\"  Saving uncertainty to: {output_spread_path}\")\n",
    "        da_std = xr.DataArray(\n",
    "            std_result,\n",
    "            coords=[('y', y_coords), ('x', x_coords)],\n",
    "            dims=('y', 'x'),\n",
    "            name='SOC_uncertainty'\n",
    "        )\n",
    "        da_std.rio.write_crs(crs, inplace=True)\n",
    "        da_std.rio.write_transform(transform, inplace=True)\n",
    "        \n",
    "        da_std.rio.to_raster(\n",
    "            output_spread_path,\n",
    "            driver='GTiff',\n",
    "            dtype=np.float32,\n",
    "            compress='LZW',\n",
    "            nodata=np.nan,\n",
    "            tiled=True,\n",
    "            blockxsize=256,\n",
    "            blockysize=256,\n",
    "            BIGTIFF='IF_SAFER'\n",
    "        )\n",
    "        print(f\"  ✓ Uncertainty saved successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error saving results: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # 最终统计\n",
    "    print(f\"\\n6. Final statistics:\")\n",
    "    total_pixels = height * width\n",
    "    \n",
    "    print(f\"  Total pixels: {total_pixels:,}\")\n",
    "    print(f\"  Predicted pixels: {total_valid_pixels:,} ({total_valid_pixels/total_pixels*100:.1f}%)\")\n",
    "    print(f\"  Missing pixels: {total_pixels-total_valid_pixels:,} ({(total_pixels-total_valid_pixels)/total_pixels*100:.1f}%)\")\n",
    "    \n",
    "    if total_valid_pixels > 0:\n",
    "        final_min = np.nanmin(mean_result)\n",
    "        final_max = np.nanmax(mean_result)\n",
    "        final_mean = np.nanmean(mean_result)\n",
    "        uncert_mean = np.nanmean(std_result)\n",
    "        \n",
    "        print(f\"  Prediction range: [{final_min:.4f}, {final_max:.4f}]\")\n",
    "        print(f\"  Prediction mean: {final_mean:.4f}\")\n",
    "        print(f\"  Uncertainty mean: {uncert_mean:.4f}\")\n",
    "        print(f\"  Uncertainty range: [{np.nanmin(std_result):.4f}, {np.nanmax(std_result):.4f}]\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 70)\n",
    "    print(\"✓ Analysis completed successfully!\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # 最终内存检查\n",
    "    print(f\"\\nFinal memory usage:\")\n",
    "    check_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3e0730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble XGBoost Pipeline for Geospatial SOC Subsoil Uncertainty Quantification Under Future Climate\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import rioxarray as rxr\n",
    "import xarray as xr\n",
    "import rasterio\n",
    "from pathlib import Path\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "\n",
    "# ====== Configuration ======\n",
    "cv_model_paths = glob.glob('E:/minmin/cv_models/SOC_top/*.json') \n",
    "input_tif_folder = 'E:/cleaned_tifs_no_extremes_iqr'\n",
    "output_prediction_path = 'E:/minmin/SOC_top_prediction.tif'\n",
    "output_spread_path = 'E:/minmin/SOC_top_uncertainty_spread.tif'\n",
    "\n",
    "\n",
    "CHUNK_SIZE = 2000 \n",
    "MAX_FEATURES_IN_MEMORY = 30 \n",
    "\n",
    "def check_memory_usage():\n",
    "    mem = psutil.virtual_memory()\n",
    "    print(f\"  Memory: {mem.used/1e9:.1f}GB / {mem.total/1e9:.1f}GB ({mem.percent}%)\")\n",
    "    return mem.percent\n",
    "\n",
    "# ====== Load Models ======\n",
    "print(\"Loading models...\")\n",
    "cv_models = []\n",
    "for i, path in enumerate(cv_model_paths):\n",
    "    try:\n",
    "        model = xgb.Booster()\n",
    "        model.load_model(path)\n",
    "        cv_models.append(model)\n",
    "        print(f\"  Model {i+1}: Loaded from {os.path.basename(path)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Model {i+1}: Failed to load - {e}\")\n",
    "\n",
    "if not cv_models:\n",
    "    raise ValueError(\"No models loaded successfully!\")\n",
    "\n",
    "# Get feature names from first model\n",
    "model_feature_names = cv_models[0].feature_names\n",
    "print(f\"\\nModel feature names ({len(model_feature_names)}):\")\n",
    "for i in range(0, min(30, len(model_feature_names)), 10):\n",
    "    print(f\"  {model_feature_names[i:i+10]}\")\n",
    "\n",
    "base_features = []\n",
    "derived_features = []\n",
    "for feat in model_feature_names:\n",
    "    if '_log' in feat or '_interaction' in feat or '_squared' in feat or '_boost' in feat or '_bins' in feat:\n",
    "        derived_features.append(feat)\n",
    "    else:\n",
    "        base_features.append(feat)\n",
    "\n",
    "print(f\"\\nFeature analysis:\")\n",
    "print(f\"  Base features: {len(base_features)}\")\n",
    "print(f\"  Derived features: {len(derived_features)}\")\n",
    "\n",
    "simple_tif_to_model = {\n",
    "    'Landuse_type': 'LUtype', 'LUtype': 'LUtype', 'LU_type': 'LUtype',\n",
    "    'Recovery_mode': 'Recovmode', 'Recovmode': 'Recovmode',\n",
    "    'BD': 't_bd', 't_bd': 't_bd',\n",
    "    'pH': 't_ph', 't_ph': 't_ph',\n",
    "    'Sand': 't_sand', 't_sand': 't_sand',\n",
    "    'Silt': 't_silt', 't_silt': 't_silt',\n",
    "    'Clay': 't_clay', 't_clay': 't_clay',\n",
    "    'Vege_type': 'Vegetype', 'Vegetype': 'Vegetype',\n",
    "    'TC': 't_oc', 't_oc': 't_oc',\n",
    "    'TN': 'TN13', 'TN13': 'TN13',\n",
    "    'TK': 'TK13', 'TK13': 'TK13',\n",
    "    'Altitude': 'Altitude', 'elevation': 'Altitude',\n",
    "    'ForestAge_TC000': 'Age_plus100', 'Age': 'Age_plus100',\n",
    "    'Lon': 'x', 'Lat': 'y',\n",
    "    'MAT':'wc_BIO1_MAT',\n",
    "    'bio2':'wc_BIO2',\n",
    "    'bio3':'wc_BIO3',\n",
    "    'bio4':'wc_BIO4',\n",
    "    'bio5':'wc_BIO5',\n",
    "    'bio6':'wc_BIO6', \n",
    "    'bio7':'wc_BIO7', \n",
    "    'bio8':'wc_BIO8', \n",
    "    'bio9':'wc_BIO9', \n",
    "    'bio10':'wc_BIO10',  \n",
    "    'bio11':'wc_BIO11', \n",
    "    'MAP':'wc_BIO12_MAP',  \n",
    "    'bio13':'wc_BIO13',\n",
    "    'bio14':'wc_BIO14',\n",
    "    'bio15':'wc_BIO15',\n",
    "    'bio16':'wc_BIO16',\n",
    "    'bio17':'wc_BIO17',\n",
    "    'bio18':'wc_BIO18',\n",
    "    'bio19':'wc_BIO19'\n",
    "\n",
    "}\n",
    "\n",
    "model_to_tif = {}\n",
    "for tif_name, model_name in simple_tif_to_model.items():\n",
    "    if model_name not in model_to_tif:\n",
    "        model_to_tif[model_name] = []\n",
    "    model_to_tif[model_name].append(tif_name)\n",
    "\n",
    "def get_reference_info(tif_folder):\n",
    "    \"\"\"获取参考栅格的详细信息\"\"\"\n",
    "    print(f\"\\n=== Getting reference raster info ===\")\n",
    "    \n",
    "    all_tif_files = list(Path(tif_folder).glob(\"*.tif\"))\n",
    "    if not all_tif_files:\n",
    "        raise ValueError(f\"No TIF files found in {tif_folder}\")\n",
    "    \n",
    "    ref_candidates = ['Recovery_mode', 'Recovmode', 'Landuse_type', 'Altitude']\n",
    "    ref_path = None\n",
    "    \n",
    "    for candidate in ref_candidates:\n",
    "        candidate_path = os.path.join(tif_folder, f\"{candidate}.tif\")\n",
    "        if os.path.exists(candidate_path):\n",
    "            ref_path = candidate_path\n",
    "            break\n",
    "    \n",
    "    if not ref_path:\n",
    "        ref_path = str(all_tif_files[0])\n",
    "    \n",
    "    print(f\"Using reference raster: {os.path.basename(ref_path)}\")\n",
    "    \n",
    "    with rasterio.open(ref_path) as src:\n",
    "        height, width = src.height, src.width\n",
    "        transform = src.transform\n",
    "        crs = src.crs\n",
    "        dtype = src.dtypes[0]\n",
    "    \n",
    "    total_pixels = height * width\n",
    "    bytes_per_pixel = 4  # float32\n",
    "    estimated_memory = len(base_features) * total_pixels * bytes_per_pixel / 1e9\n",
    "    \n",
    "    print(f\"  Shape: {height} x {width} = {total_pixels:,} pixels\")\n",
    "    print(f\"  Data type: {dtype}\")\n",
    "    print(f\"  CRS: {crs}\")\n",
    "    print(f\"  Estimated memory for base features: {estimated_memory:.2f} GB\")\n",
    "    print(f\"  Chunk size: {CHUNK_SIZE} rows\")\n",
    "    print(f\"  Pixels per chunk: {CHUNK_SIZE * width:,}\")\n",
    "    \n",
    "    return height, width, transform, crs\n",
    "\n",
    "def preload_feature_metadata(tif_folder, model_features):\n",
    "    \"\"\"预加载特征文件的路径和元数据\"\"\"\n",
    "    print(f\"\\n=== Preloading feature metadata ===\")\n",
    "    \n",
    "    all_tif_files = list(Path(tif_folder).glob(\"*.tif\"))\n",
    "    tif_names = [f.stem for f in all_tif_files]\n",
    "    \n",
    "    feature_metadata = {}\n",
    "    \n",
    "    for model_feat in base_features: \n",
    "        possible_tif_names = model_to_tif.get(model_feat, [model_feat])\n",
    "        tif_path = None\n",
    "        \n",
    "        for tif_name in possible_tif_names:\n",
    "            if tif_name in tif_names:\n",
    "                tif_path = os.path.join(tif_folder, f\"{tif_name}.tif\")\n",
    "                break\n",
    "        \n",
    "        if tif_path and os.path.exists(tif_path):\n",
    "            try:\n",
    "                with rasterio.open(tif_path) as src:\n",
    "                    feature_metadata[model_feat] = {\n",
    "                        'path': tif_path,\n",
    "                        'dtype': src.dtypes[0],\n",
    "                        'nodata': src.nodata\n",
    "                    }\n",
    "                    print(f\"  ✓ {model_feat} -> {os.path.basename(tif_path)}\")\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    print(f\"  Found metadata for {len(feature_metadata)} base features\")\n",
    "    return feature_metadata\n",
    "\n",
    "def load_chunk_data_efficient(feature_metadata, row_start, row_end, width, transform):\n",
    "    \"\"\"高效加载分块数据\"\"\"\n",
    "    chunk_data = {}\n",
    "    chunk_height = row_end - row_start\n",
    "    \n",
    "    if chunk_height <= 0:\n",
    "        return chunk_data\n",
    "    \n",
    "    for model_feat, meta in feature_metadata.items():\n",
    "        try:\n",
    "            with rasterio.open(meta['path']) as src:\n",
    "                window = ((row_start, row_end), (0, width))\n",
    "                data = src.read(1, window=window)\n",
    "                \n",
    "                data = data.astype(np.float32)\n",
    "                if meta['nodata'] is not None:\n",
    "                    data[data == meta['nodata']] = np.nan\n",
    "                \n",
    "                chunk_data[model_feat] = data\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"    Error loading {model_feat}: {e}\")\n",
    "            chunk_data[model_feat] = np.full((chunk_height, width), np.nan, dtype=np.float32)\n",
    "    \n",
    "    if 'x' in base_features:\n",
    "        x_coords = np.arange(width) * transform[0] + transform[2] + transform[0] / 2\n",
    "        chunk_data['x'] = np.tile(x_coords, (chunk_height, 1)).astype(np.float32)\n",
    "    \n",
    "    if 'y' in base_features:\n",
    "        y_coords = np.arange(row_start, row_end) * transform[4] + transform[5] + transform[4] / 2\n",
    "        chunk_data['y'] = np.tile(y_coords.reshape(-1, 1), (1, width)).astype(np.float32)\n",
    "    \n",
    "    return chunk_data\n",
    "\n",
    "def create_derived_features_batch(chunk_data, model_features):\n",
    "    processed = chunk_data.copy()\n",
    "    \n",
    "    if not processed:\n",
    "        return processed\n",
    "    \n",
    "    chunk_rows, chunk_cols = next(iter(processed.values())).shape\n",
    "    \n",
    "    log_transforms = [\n",
    "        ('x', 'Lon_log'), ('y', 'Lat_log'), \n",
    "        ('Age', 'Age_log'), ('s_bd', 'BD_log'), ('s_ph', 'pH_log')\n",
    "    ]\n",
    "    \n",
    "    for base_feat, log_feat in log_transforms:\n",
    "        if log_feat in model_features and base_feat in processed:\n",
    "            data = processed[base_feat].copy()\n",
    "            mask = ~np.isnan(data)\n",
    "            if np.any(mask):\n",
    "                data[mask] = np.log(data[mask] + 1e-8)\n",
    "            processed[log_feat] = data\n",
    "    \n",
    "    if 'LUtype' in processed:\n",
    "        lu_data = processed['LUtype'].copy()\n",
    "        lu_filled = np.where(np.isnan(lu_data), 0, lu_data)\n",
    "        \n",
    "        lu_boost_features = [f for f in model_features if f.startswith('LUtype_boost_')]\n",
    "        for boost_feat in lu_boost_features:\n",
    "            try:\n",
    "                boost_num = int(boost_feat.split('_')[-1])\n",
    "                processed[boost_feat] = lu_filled * boost_num if boost_num > 1 else lu_filled\n",
    "            except:\n",
    "                processed[boost_feat] = lu_filled\n",
    "        \n",
    "        if 'LUtype_squared' in model_features:\n",
    "            processed['LUtype_squared'] = lu_filled ** 2\n",
    "        \n",
    "        for other_feat in ['s_ph', 's_bd', 'Age', 'Altitude']:\n",
    "            interaction_name = f'LUtype_{other_feat}_interaction'\n",
    "            if interaction_name in model_features and other_feat in processed:\n",
    "                other_data = processed[other_feat].copy()\n",
    "                other_filled = np.where(np.isnan(other_data), 0, other_data)\n",
    "                processed[interaction_name] = lu_filled * other_filled\n",
    "    \n",
    "    if 'Altitude_bins' in model_features and 'Altitude' in processed:\n",
    "        alt_data = processed['Altitude'].copy()\n",
    "        alt_filled = np.where(np.isnan(alt_data), 0, alt_data)\n",
    "        processed['Altitude_bins'] = alt_filled\n",
    "    \n",
    "    missing = [f for f in model_features if f not in processed]\n",
    "    if missing:\n",
    "        for feat in missing:\n",
    "            processed[feat] = np.zeros((chunk_rows, chunk_cols), dtype=np.float32)\n",
    "    \n",
    "    return processed\n",
    "\n",
    "def predict_chunk_batch(chunk_data, models):\n",
    "    \"\"\"对分块数据进行批量预测\"\"\"\n",
    "    if not models or not chunk_data:\n",
    "        return None, None\n",
    "    \n",
    "    model_features = models[0].feature_names\n",
    "    \n",
    "    all_features = create_derived_features_batch(chunk_data, model_features)\n",
    "    \n",
    "    chunk_rows, chunk_cols = next(iter(all_features.values())).shape\n",
    "\n",
    "    core_features = ['Recovmode', 'Recovery_mode', 'LUtype', 'Altitude']\n",
    "    core_feature = None\n",
    "    for cf in core_features:\n",
    "        if cf in all_features:\n",
    "            core_feature = cf\n",
    "            break\n",
    "    \n",
    "    if core_feature:\n",
    "        valid_mask = ~np.isnan(all_features[core_feature])\n",
    "    else:\n",
    "        valid_mask = np.ones((chunk_rows, chunk_cols), dtype=bool)\n",
    "    \n",
    "    feature_arrays = []\n",
    "    for feat in model_features:\n",
    "        if feat in all_features:\n",
    "            feature_arrays.append(all_features[feat])\n",
    "        else:\n",
    "            feature_arrays.append(np.zeros((chunk_rows, chunk_cols), dtype=np.float32))\n",
    "    \n",
    "    X_3d = np.stack(feature_arrays, axis=-1)\n",
    "    X_flat = X_3d.reshape(-1, len(model_features))\n",
    "    \n",
    "    valid_indices = valid_mask.flatten()\n",
    "    X_valid = X_flat[valid_indices]\n",
    "    \n",
    "    if len(X_valid) == 0:\n",
    "        return None, None\n",
    "    \n",
    "    all_predictions = []\n",
    "    for model in models:\n",
    "        dmatrix = xgb.DMatrix(X_valid, feature_names=model_features)\n",
    "        preds = model.predict(dmatrix)\n",
    "        all_predictions.append(preds)\n",
    "    \n",
    "    preds_array = np.array(all_predictions)\n",
    "    mean_pred = np.mean(preds_array, axis=0)\n",
    "    std_pred = np.std(preds_array, axis=0)\n",
    "\n",
    "    mean_full = np.full(chunk_rows * chunk_cols, np.nan, dtype=np.float32)\n",
    "    std_full = np.full(chunk_rows * chunk_cols, np.nan, dtype=np.float32)\n",
    "    \n",
    "    mean_full[valid_indices] = mean_pred\n",
    "    std_full[valid_indices] = std_pred\n",
    "    \n",
    "    return mean_full.reshape(chunk_rows, chunk_cols), std_full.reshape(chunk_rows, chunk_cols)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 70)\n",
    "    print(\"SOC Active Layer Prediction - Optimized for Large Memory Systems\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\nInitial system check:\")\n",
    "    check_memory_usage()\n",
    "\n",
    "    print(f\"\\n1. Loading {len(cv_model_paths)} models...\")\n",
    "\n",
    "    print(f\"\\n2. Analyzing input data...\")\n",
    "    height, width, transform, crs = get_reference_info(input_tif_folder)\n",
    "    \n",
    "    print(f\"\\n3. Preloading feature metadata...\")\n",
    "    feature_metadata = preload_feature_metadata(input_tif_folder, model_feature_names)\n",
    "    \n",
    "    if not feature_metadata:\n",
    "        raise ValueError(\"No base features found! Check your TIF files.\")\n",
    "    \n",
    "    print(f\"\\n4. Starting chunked prediction...\")\n",
    "    \n",
    "    mean_result = np.full((height, width), np.nan, dtype=np.float32)\n",
    "    std_result = np.full((height, width), np.nan, dtype=np.float32)\n",
    "    \n",
    "    total_chunks = int(np.ceil(height / CHUNK_SIZE))\n",
    "    total_valid_pixels = 0\n",
    "    \n",
    "    print(f\"\\nProgress:\")\n",
    "    \n",
    "    for chunk_idx in range(total_chunks):\n",
    "        chunk_start = chunk_idx * CHUNK_SIZE\n",
    "        chunk_end = min(chunk_start + CHUNK_SIZE, height)\n",
    "        chunk_height = chunk_end - chunk_start\n",
    "        \n",
    "        print(f\"\\n  Chunk {chunk_idx+1}/{total_chunks}: Rows {chunk_start:,}-{chunk_end:,}\")\n",
    "        check_memory_usage()\n",
    "        \n",
    "        chunk_data = load_chunk_data_efficient(\n",
    "            feature_metadata, chunk_start, chunk_end, width, transform\n",
    "        )\n",
    "        \n",
    "        if not chunk_data:\n",
    "            print(f\"    No data loaded, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"    Loaded {len(chunk_data)} features for this chunk\")\n",
    "\n",
    "        mean_chunk, std_chunk = predict_chunk_batch(chunk_data, cv_models)\n",
    "        \n",
    "        if mean_chunk is not None:\n",
    "            mean_result[chunk_start:chunk_end, :] = mean_chunk\n",
    "            std_result[chunk_start:chunk_end, :] = std_chunk\n",
    "            valid_in_chunk = np.sum(~np.isnan(mean_chunk))\n",
    "            total_valid_pixels += valid_in_chunk\n",
    "            \n",
    "            chunk_min = np.nanmin(mean_chunk)\n",
    "            chunk_max = np.nanmax(mean_chunk)\n",
    "            print(f\"    Predicted: {valid_in_chunk:,} pixels\")\n",
    "            print(f\"    Range: [{chunk_min:.3f}, {chunk_max:.3f}]\")\n",
    "            print(f\"    Progress: {total_valid_pixels/(height*width)*100:.1f}%\")\n",
    "        del chunk_data, mean_chunk, std_chunk\n",
    "        gc.collect()\n",
    "    print(f\"\\n5. Saving results...\")\n",
    "    \n",
    "    try:\n",
    "        x_coords = np.arange(width) * transform[0] + transform[2] + transform[0] / 2\n",
    "        y_coords = np.arange(height) * transform[4] + transform[5] + transform[4] / 2\n",
    "        print(f\"  Saving prediction to: {output_prediction_path}\")\n",
    "        da_mean = xr.DataArray(\n",
    "            mean_result,\n",
    "            coords=[('y', y_coords), ('x', x_coords)],\n",
    "            dims=('y', 'x'),\n",
    "            name='SOC_prediction'\n",
    "        )\n",
    "        da_mean.rio.write_crs(crs, inplace=True)\n",
    "        da_mean.rio.write_transform(transform, inplace=True)\n",
    "        \n",
    "        da_mean.rio.to_raster(\n",
    "            output_prediction_path,\n",
    "            driver='GTiff',\n",
    "            dtype=np.float32,\n",
    "            compress='LZW',\n",
    "            nodata=np.nan,\n",
    "            tiled=True,\n",
    "            blockxsize=256,\n",
    "            blockysize=256,\n",
    "            BIGTIFF='IF_SAFER' \n",
    "        )\n",
    "        print(f\"  ✓ Prediction saved successfully\")\n",
    "        \n",
    "        print(f\"  Saving uncertainty to: {output_spread_path}\")\n",
    "        da_std = xr.DataArray(\n",
    "            std_result,\n",
    "            coords=[('y', y_coords), ('x', x_coords)],\n",
    "            dims=('y', 'x'),\n",
    "            name='SOC_uncertainty'\n",
    "        )\n",
    "        da_std.rio.write_crs(crs, inplace=True)\n",
    "        da_std.rio.write_transform(transform, inplace=True)\n",
    "        \n",
    "        da_std.rio.to_raster(\n",
    "            output_spread_path,\n",
    "            driver='GTiff',\n",
    "            dtype=np.float32,\n",
    "            compress='LZW',\n",
    "            nodata=np.nan,\n",
    "            tiled=True,\n",
    "            blockxsize=256,\n",
    "            blockysize=256,\n",
    "            BIGTIFF='IF_SAFER'\n",
    "        )\n",
    "        print(f\"  ✓ Uncertainty saved successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error saving results: {e}\")\n",
    "        raise\n",
    "\n",
    "    print(f\"\\n6. Final statistics:\")\n",
    "    total_pixels = height * width\n",
    "    \n",
    "    print(f\"  Total pixels: {total_pixels:,}\")\n",
    "    print(f\"  Predicted pixels: {total_valid_pixels:,} ({total_valid_pixels/total_pixels*100:.1f}%)\")\n",
    "    print(f\"  Missing pixels: {total_pixels-total_valid_pixels:,} ({(total_pixels-total_valid_pixels)/total_pixels*100:.1f}%)\")\n",
    "    \n",
    "    if total_valid_pixels > 0:\n",
    "        final_min = np.nanmin(mean_result)\n",
    "        final_max = np.nanmax(mean_result)\n",
    "        final_mean = np.nanmean(mean_result)\n",
    "        uncert_mean = np.nanmean(std_result)\n",
    "        \n",
    "        print(f\"  Prediction range: [{final_min:.4f}, {final_max:.4f}]\")\n",
    "        print(f\"  Prediction mean: {final_mean:.4f}\")\n",
    "        print(f\"  Uncertainty mean: {uncert_mean:.4f}\")\n",
    "        print(f\"  Uncertainty range: [{np.nanmin(std_result):.4f}, {np.nanmax(std_result):.4f}]\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 70)\n",
    "    print(\"✓ Analysis completed successfully!\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # 最终内存检查\n",
    "    print(f\"\\nFinal memory usage:\")\n",
    "    check_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f656fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble XGBoost Pipeline for Geospatial SOC Subsoil Uncertainty Quantification Under Future Climate\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import rioxarray as rxr\n",
    "import xarray as xr\n",
    "import rasterio\n",
    "from pathlib import Path\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "\n",
    "# ====== Configuration ======\n",
    "cv_model_paths = glob.glob('E:/minmin/cv_models/SOC_sub/*.json') \n",
    "input_tif_folder = 'E:/cleaned_tifs_no_extremes_iqr'\n",
    "output_prediction_path = 'E:/minmin/SOC_sub_prediction.tif'\n",
    "output_spread_path = 'E:/minmin/SOC_sub_uncertainty_spread.tif'\n",
    "\n",
    "\n",
    "CHUNK_SIZE = 2000 \n",
    "MAX_FEATURES_IN_MEMORY = 30 \n",
    "\n",
    "def check_memory_usage():\n",
    "    mem = psutil.virtual_memory()\n",
    "    print(f\"  Memory: {mem.used/1e9:.1f}GB / {mem.total/1e9:.1f}GB ({mem.percent}%)\")\n",
    "    return mem.percent\n",
    "\n",
    "# ====== Load Models ======\n",
    "print(\"Loading models...\")\n",
    "cv_models = []\n",
    "for i, path in enumerate(cv_model_paths):\n",
    "    try:\n",
    "        model = xgb.Booster()\n",
    "        model.load_model(path)\n",
    "        cv_models.append(model)\n",
    "        print(f\"  Model {i+1}: Loaded from {os.path.basename(path)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Model {i+1}: Failed to load - {e}\")\n",
    "\n",
    "if not cv_models:\n",
    "    raise ValueError(\"No models loaded successfully!\")\n",
    "\n",
    "# Get feature names from first model\n",
    "model_feature_names = cv_models[0].feature_names\n",
    "print(f\"\\nModel feature names ({len(model_feature_names)}):\")\n",
    "for i in range(0, min(30, len(model_feature_names)), 10):\n",
    "    print(f\"  {model_feature_names[i:i+10]}\")\n",
    "\n",
    "base_features = []\n",
    "derived_features = []\n",
    "for feat in model_feature_names:\n",
    "    if '_log' in feat or '_interaction' in feat or '_squared' in feat or '_boost' in feat or '_bins' in feat:\n",
    "        derived_features.append(feat)\n",
    "    else:\n",
    "        base_features.append(feat)\n",
    "\n",
    "print(f\"\\nFeature analysis:\")\n",
    "print(f\"  Base features: {len(base_features)}\")\n",
    "print(f\"  Derived features: {len(derived_features)}\")\n",
    "\n",
    "simple_tif_to_model = {\n",
    "    'Landuse_type': 'LUtype', 'LUtype': 'LUtype', 'LU_type': 'LUtype',\n",
    "    'Recovery_mode': 'Recovmode', 'Recovmode': 'Recovmode',\n",
    "    'BD': 's_bd', 's_bd': 's_bd',\n",
    "    'pH': 's_ph', 's_ph': 's_ph',\n",
    "    'Sand': 's_sand', 's_sand': 's_sand',\n",
    "    'Silt': 's_silt', 's_silt': 's_silt',\n",
    "    'Clay': 's_clay', 's_clay': 's_clay',\n",
    "    'Vege_type': 'Vegetype', 'Vegetype': 'Vegetype',\n",
    "    'TC': 's_oc', 's_oc': 's_oc',\n",
    "    'TN': 'TN46', 'TN46': 'TN46',\n",
    "    'TK': 'TK46', 'TK46': 'TK46',\n",
    "    'Altitude': 'Altitude', 'elevation': 'Altitude',\n",
    "    'ForestAge_TC000': 'Age_plus100', 'Age': 'Age_plus100',\n",
    "    'Lon': 'x', 'Lat': 'y',\n",
    "    'MAT':'wc_BIO1_MAT',\n",
    "    'bio2':'wc_BIO2',\n",
    "    'bio3':'wc_BIO3',\n",
    "    'bio4':'wc_BIO4',\n",
    "    'bio5':'wc_BIO5',\n",
    "    'bio6':'wc_BIO6', \n",
    "    'bio7':'wc_BIO7', \n",
    "    'bio8':'wc_BIO8', \n",
    "    'bio9':'wc_BIO9', \n",
    "    'bio10':'wc_BIO10',  \n",
    "    'bio11':'wc_BIO11', \n",
    "    'MAP':'wc_BIO12_MAP',  \n",
    "    'bio13':'wc_BIO13',\n",
    "    'bio14':'wc_BIO14',\n",
    "    'bio15':'wc_BIO15',\n",
    "    'bio16':'wc_BIO16',\n",
    "    'bio17':'wc_BIO17',\n",
    "    'bio18':'wc_BIO18',\n",
    "    'bio19':'wc_BIO19'\n",
    "\n",
    "}\n",
    "\n",
    "model_to_tif = {}\n",
    "for tif_name, model_name in simple_tif_to_model.items():\n",
    "    if model_name not in model_to_tif:\n",
    "        model_to_tif[model_name] = []\n",
    "    model_to_tif[model_name].append(tif_name)\n",
    "\n",
    "def get_reference_info(tif_folder):\n",
    "    \"\"\"获取参考栅格的详细信息\"\"\"\n",
    "    print(f\"\\n=== Getting reference raster info ===\")\n",
    "    \n",
    "    all_tif_files = list(Path(tif_folder).glob(\"*.tif\"))\n",
    "    if not all_tif_files:\n",
    "        raise ValueError(f\"No TIF files found in {tif_folder}\")\n",
    "    \n",
    "    ref_candidates = ['Recovery_mode', 'Recovmode', 'Landuse_type', 'Altitude']\n",
    "    ref_path = None\n",
    "    \n",
    "    for candidate in ref_candidates:\n",
    "        candidate_path = os.path.join(tif_folder, f\"{candidate}.tif\")\n",
    "        if os.path.exists(candidate_path):\n",
    "            ref_path = candidate_path\n",
    "            break\n",
    "    \n",
    "    if not ref_path:\n",
    "        ref_path = str(all_tif_files[0])\n",
    "    \n",
    "    print(f\"Using reference raster: {os.path.basename(ref_path)}\")\n",
    "    \n",
    "    with rasterio.open(ref_path) as src:\n",
    "        height, width = src.height, src.width\n",
    "        transform = src.transform\n",
    "        crs = src.crs\n",
    "        dtype = src.dtypes[0]\n",
    "    \n",
    "    total_pixels = height * width\n",
    "    bytes_per_pixel = 4  # float32\n",
    "    estimated_memory = len(base_features) * total_pixels * bytes_per_pixel / 1e9\n",
    "    \n",
    "    print(f\"  Shape: {height} x {width} = {total_pixels:,} pixels\")\n",
    "    print(f\"  Data type: {dtype}\")\n",
    "    print(f\"  CRS: {crs}\")\n",
    "    print(f\"  Estimated memory for base features: {estimated_memory:.2f} GB\")\n",
    "    print(f\"  Chunk size: {CHUNK_SIZE} rows\")\n",
    "    print(f\"  Pixels per chunk: {CHUNK_SIZE * width:,}\")\n",
    "    \n",
    "    return height, width, transform, crs\n",
    "\n",
    "def preload_feature_metadata(tif_folder, model_features):\n",
    "    \"\"\"预加载特征文件的路径和元数据\"\"\"\n",
    "    print(f\"\\n=== Preloading feature metadata ===\")\n",
    "    \n",
    "    all_tif_files = list(Path(tif_folder).glob(\"*.tif\"))\n",
    "    tif_names = [f.stem for f in all_tif_files]\n",
    "    \n",
    "    feature_metadata = {}\n",
    "    \n",
    "    for model_feat in base_features: \n",
    "        possible_tif_names = model_to_tif.get(model_feat, [model_feat])\n",
    "        tif_path = None\n",
    "        \n",
    "        for tif_name in possible_tif_names:\n",
    "            if tif_name in tif_names:\n",
    "                tif_path = os.path.join(tif_folder, f\"{tif_name}.tif\")\n",
    "                break\n",
    "        \n",
    "        if tif_path and os.path.exists(tif_path):\n",
    "            try:\n",
    "                with rasterio.open(tif_path) as src:\n",
    "                    feature_metadata[model_feat] = {\n",
    "                        'path': tif_path,\n",
    "                        'dtype': src.dtypes[0],\n",
    "                        'nodata': src.nodata\n",
    "                    }\n",
    "                    print(f\"  ✓ {model_feat} -> {os.path.basename(tif_path)}\")\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    print(f\"  Found metadata for {len(feature_metadata)} base features\")\n",
    "    return feature_metadata\n",
    "\n",
    "def load_chunk_data_efficient(feature_metadata, row_start, row_end, width, transform):\n",
    "    \"\"\"高效加载分块数据\"\"\"\n",
    "    chunk_data = {}\n",
    "    chunk_height = row_end - row_start\n",
    "    \n",
    "    if chunk_height <= 0:\n",
    "        return chunk_data\n",
    "    \n",
    "    for model_feat, meta in feature_metadata.items():\n",
    "        try:\n",
    "            with rasterio.open(meta['path']) as src:\n",
    "                window = ((row_start, row_end), (0, width))\n",
    "                data = src.read(1, window=window)\n",
    "                \n",
    "                data = data.astype(np.float32)\n",
    "                if meta['nodata'] is not None:\n",
    "                    data[data == meta['nodata']] = np.nan\n",
    "                \n",
    "                chunk_data[model_feat] = data\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"    Error loading {model_feat}: {e}\")\n",
    "            chunk_data[model_feat] = np.full((chunk_height, width), np.nan, dtype=np.float32)\n",
    "    \n",
    "    if 'x' in base_features:\n",
    "        x_coords = np.arange(width) * transform[0] + transform[2] + transform[0] / 2\n",
    "        chunk_data['x'] = np.tile(x_coords, (chunk_height, 1)).astype(np.float32)\n",
    "    \n",
    "    if 'y' in base_features:\n",
    "        y_coords = np.arange(row_start, row_end) * transform[4] + transform[5] + transform[4] / 2\n",
    "        chunk_data['y'] = np.tile(y_coords.reshape(-1, 1), (1, width)).astype(np.float32)\n",
    "    \n",
    "    return chunk_data\n",
    "\n",
    "def create_derived_features_batch(chunk_data, model_features):\n",
    "    processed = chunk_data.copy()\n",
    "    \n",
    "    if not processed:\n",
    "        return processed\n",
    "    \n",
    "    chunk_rows, chunk_cols = next(iter(processed.values())).shape\n",
    "    \n",
    "    log_transforms = [\n",
    "        ('x', 'Lon_log'), ('y', 'Lat_log'), \n",
    "        ('Age', 'Age_log'), ('s_bd', 'BD_log'), ('s_ph', 'pH_log')\n",
    "    ]\n",
    "    \n",
    "    for base_feat, log_feat in log_transforms:\n",
    "        if log_feat in model_features and base_feat in processed:\n",
    "            data = processed[base_feat].copy()\n",
    "            mask = ~np.isnan(data)\n",
    "            if np.any(mask):\n",
    "                data[mask] = np.log(data[mask] + 1e-8)\n",
    "            processed[log_feat] = data\n",
    "    \n",
    "    if 'LUtype' in processed:\n",
    "        lu_data = processed['LUtype'].copy()\n",
    "        lu_filled = np.where(np.isnan(lu_data), 0, lu_data)\n",
    "        \n",
    "        lu_boost_features = [f for f in model_features if f.startswith('LUtype_boost_')]\n",
    "        for boost_feat in lu_boost_features:\n",
    "            try:\n",
    "                boost_num = int(boost_feat.split('_')[-1])\n",
    "                processed[boost_feat] = lu_filled * boost_num if boost_num > 1 else lu_filled\n",
    "            except:\n",
    "                processed[boost_feat] = lu_filled\n",
    "        \n",
    "        if 'LUtype_squared' in model_features:\n",
    "            processed['LUtype_squared'] = lu_filled ** 2\n",
    "        \n",
    "        for other_feat in ['s_ph', 's_bd', 'Age', 'Altitude']:\n",
    "            interaction_name = f'LUtype_{other_feat}_interaction'\n",
    "            if interaction_name in model_features and other_feat in processed:\n",
    "                other_data = processed[other_feat].copy()\n",
    "                other_filled = np.where(np.isnan(other_data), 0, other_data)\n",
    "                processed[interaction_name] = lu_filled * other_filled\n",
    "    \n",
    "    if 'Altitude_bins' in model_features and 'Altitude' in processed:\n",
    "        alt_data = processed['Altitude'].copy()\n",
    "        alt_filled = np.where(np.isnan(alt_data), 0, alt_data)\n",
    "        processed['Altitude_bins'] = alt_filled\n",
    "    \n",
    "    missing = [f for f in model_features if f not in processed]\n",
    "    if missing:\n",
    "        for feat in missing:\n",
    "            processed[feat] = np.zeros((chunk_rows, chunk_cols), dtype=np.float32)\n",
    "    \n",
    "    return processed\n",
    "\n",
    "def predict_chunk_batch(chunk_data, models):\n",
    "    \"\"\"对分块数据进行批量预测\"\"\"\n",
    "    if not models or not chunk_data:\n",
    "        return None, None\n",
    "    \n",
    "    model_features = models[0].feature_names\n",
    "    \n",
    "    all_features = create_derived_features_batch(chunk_data, model_features)\n",
    "    \n",
    "    chunk_rows, chunk_cols = next(iter(all_features.values())).shape\n",
    "\n",
    "    core_features = ['Recovmode', 'Recovery_mode', 'LUtype', 'Altitude']\n",
    "    core_feature = None\n",
    "    for cf in core_features:\n",
    "        if cf in all_features:\n",
    "            core_feature = cf\n",
    "            break\n",
    "    \n",
    "    if core_feature:\n",
    "        valid_mask = ~np.isnan(all_features[core_feature])\n",
    "    else:\n",
    "        valid_mask = np.ones((chunk_rows, chunk_cols), dtype=bool)\n",
    "    \n",
    "    feature_arrays = []\n",
    "    for feat in model_features:\n",
    "        if feat in all_features:\n",
    "            feature_arrays.append(all_features[feat])\n",
    "        else:\n",
    "            feature_arrays.append(np.zeros((chunk_rows, chunk_cols), dtype=np.float32))\n",
    "    \n",
    "    X_3d = np.stack(feature_arrays, axis=-1)\n",
    "    X_flat = X_3d.reshape(-1, len(model_features))\n",
    "    \n",
    "    valid_indices = valid_mask.flatten()\n",
    "    X_valid = X_flat[valid_indices]\n",
    "    \n",
    "    if len(X_valid) == 0:\n",
    "        return None, None\n",
    "    \n",
    "    all_predictions = []\n",
    "    for model in models:\n",
    "        dmatrix = xgb.DMatrix(X_valid, feature_names=model_features)\n",
    "        preds = model.predict(dmatrix)\n",
    "        all_predictions.append(preds)\n",
    "    \n",
    "    preds_array = np.array(all_predictions)\n",
    "    mean_pred = np.mean(preds_array, axis=0)\n",
    "    std_pred = np.std(preds_array, axis=0)\n",
    "\n",
    "    mean_full = np.full(chunk_rows * chunk_cols, np.nan, dtype=np.float32)\n",
    "    std_full = np.full(chunk_rows * chunk_cols, np.nan, dtype=np.float32)\n",
    "    \n",
    "    mean_full[valid_indices] = mean_pred\n",
    "    std_full[valid_indices] = std_pred\n",
    "    \n",
    "    return mean_full.reshape(chunk_rows, chunk_cols), std_full.reshape(chunk_rows, chunk_cols)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 70)\n",
    "    print(\"SOC Active Layer Prediction - Optimized for Large Memory Systems\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\nInitial system check:\")\n",
    "    check_memory_usage()\n",
    "\n",
    "    print(f\"\\n1. Loading {len(cv_model_paths)} models...\")\n",
    "\n",
    "    print(f\"\\n2. Analyzing input data...\")\n",
    "    height, width, transform, crs = get_reference_info(input_tif_folder)\n",
    "    \n",
    "    print(f\"\\n3. Preloading feature metadata...\")\n",
    "    feature_metadata = preload_feature_metadata(input_tif_folder, model_feature_names)\n",
    "    \n",
    "    if not feature_metadata:\n",
    "        raise ValueError(\"No base features found! Check your TIF files.\")\n",
    "    \n",
    "    print(f\"\\n4. Starting chunked prediction...\")\n",
    "    \n",
    "    mean_result = np.full((height, width), np.nan, dtype=np.float32)\n",
    "    std_result = np.full((height, width), np.nan, dtype=np.float32)\n",
    "    \n",
    "    total_chunks = int(np.ceil(height / CHUNK_SIZE))\n",
    "    total_valid_pixels = 0\n",
    "    \n",
    "    print(f\"\\nProgress:\")\n",
    "    \n",
    "    for chunk_idx in range(total_chunks):\n",
    "        chunk_start = chunk_idx * CHUNK_SIZE\n",
    "        chunk_end = min(chunk_start + CHUNK_SIZE, height)\n",
    "        chunk_height = chunk_end - chunk_start\n",
    "        \n",
    "        print(f\"\\n  Chunk {chunk_idx+1}/{total_chunks}: Rows {chunk_start:,}-{chunk_end:,}\")\n",
    "        check_memory_usage()\n",
    "        \n",
    "        chunk_data = load_chunk_data_efficient(\n",
    "            feature_metadata, chunk_start, chunk_end, width, transform\n",
    "        )\n",
    "        \n",
    "        if not chunk_data:\n",
    "            print(f\"    No data loaded, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"    Loaded {len(chunk_data)} features for this chunk\")\n",
    "\n",
    "        mean_chunk, std_chunk = predict_chunk_batch(chunk_data, cv_models)\n",
    "        \n",
    "        if mean_chunk is not None:\n",
    "            mean_result[chunk_start:chunk_end, :] = mean_chunk\n",
    "            std_result[chunk_start:chunk_end, :] = std_chunk\n",
    "            valid_in_chunk = np.sum(~np.isnan(mean_chunk))\n",
    "            total_valid_pixels += valid_in_chunk\n",
    "            \n",
    "            chunk_min = np.nanmin(mean_chunk)\n",
    "            chunk_max = np.nanmax(mean_chunk)\n",
    "            print(f\"    Predicted: {valid_in_chunk:,} pixels\")\n",
    "            print(f\"    Range: [{chunk_min:.3f}, {chunk_max:.3f}]\")\n",
    "            print(f\"    Progress: {total_valid_pixels/(height*width)*100:.1f}%\")\n",
    "        del chunk_data, mean_chunk, std_chunk\n",
    "        gc.collect()\n",
    "    print(f\"\\n5. Saving results...\")\n",
    "    \n",
    "    try:\n",
    "        x_coords = np.arange(width) * transform[0] + transform[2] + transform[0] / 2\n",
    "        y_coords = np.arange(height) * transform[4] + transform[5] + transform[4] / 2\n",
    "        print(f\"  Saving prediction to: {output_prediction_path}\")\n",
    "        da_mean = xr.DataArray(\n",
    "            mean_result,\n",
    "            coords=[('y', y_coords), ('x', x_coords)],\n",
    "            dims=('y', 'x'),\n",
    "            name='SOC_prediction'\n",
    "        )\n",
    "        da_mean.rio.write_crs(crs, inplace=True)\n",
    "        da_mean.rio.write_transform(transform, inplace=True)\n",
    "        \n",
    "        da_mean.rio.to_raster(\n",
    "            output_prediction_path,\n",
    "            driver='GTiff',\n",
    "            dtype=np.float32,\n",
    "            compress='LZW',\n",
    "            nodata=np.nan,\n",
    "            tiled=True,\n",
    "            blockxsize=256,\n",
    "            blockysize=256,\n",
    "            BIGTIFF='IF_SAFER' \n",
    "        )\n",
    "        print(f\"  ✓ Prediction saved successfully\")\n",
    "        \n",
    "        print(f\"  Saving uncertainty to: {output_spread_path}\")\n",
    "        da_std = xr.DataArray(\n",
    "            std_result,\n",
    "            coords=[('y', y_coords), ('x', x_coords)],\n",
    "            dims=('y', 'x'),\n",
    "            name='SOC_uncertainty'\n",
    "        )\n",
    "        da_std.rio.write_crs(crs, inplace=True)\n",
    "        da_std.rio.write_transform(transform, inplace=True)\n",
    "        \n",
    "        da_std.rio.to_raster(\n",
    "            output_spread_path,\n",
    "            driver='GTiff',\n",
    "            dtype=np.float32,\n",
    "            compress='LZW',\n",
    "            nodata=np.nan,\n",
    "            tiled=True,\n",
    "            blockxsize=256,\n",
    "            blockysize=256,\n",
    "            BIGTIFF='IF_SAFER'\n",
    "        )\n",
    "        print(f\"  ✓ Uncertainty saved successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error saving results: {e}\")\n",
    "        raise\n",
    "\n",
    "    print(f\"\\n6. Final statistics:\")\n",
    "    total_pixels = height * width\n",
    "    \n",
    "    print(f\"  Total pixels: {total_pixels:,}\")\n",
    "    print(f\"  Predicted pixels: {total_valid_pixels:,} ({total_valid_pixels/total_pixels*100:.1f}%)\")\n",
    "    print(f\"  Missing pixels: {total_pixels-total_valid_pixels:,} ({(total_pixels-total_valid_pixels)/total_pixels*100:.1f}%)\")\n",
    "    \n",
    "    if total_valid_pixels > 0:\n",
    "        final_min = np.nanmin(mean_result)\n",
    "        final_max = np.nanmax(mean_result)\n",
    "        final_mean = np.nanmean(mean_result)\n",
    "        uncert_mean = np.nanmean(std_result)\n",
    "        \n",
    "        print(f\"  Prediction range: [{final_min:.4f}, {final_max:.4f}]\")\n",
    "        print(f\"  Prediction mean: {final_mean:.4f}\")\n",
    "        print(f\"  Uncertainty mean: {uncert_mean:.4f}\")\n",
    "        print(f\"  Uncertainty range: [{np.nanmin(std_result):.4f}, {np.nanmax(std_result):.4f}]\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 70)\n",
    "    print(\"✓ Analysis completed successfully!\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # 最终内存检查\n",
    "    print(f\"\\nFinal memory usage:\")\n",
    "    check_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127aed9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Raster-Based Coefficient of Variation (CV) Generator for Uncertainty Analysis\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import os\n",
    "from rasterio.plot import show\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_cv(mean_path, sd_path, output_path=None):\n",
    "    \"\"\"\n",
    "    Calculate CV = (SD / Mean) × 100%, and SD/Mean take absolute values.\n",
    "    \"\"\"\n",
    "    \n",
    "    with rasterio.open(mean_path) as mean_src, \\\n",
    "         rasterio.open(sd_path) as sd_src:\n",
    "        \n",
    "        mean_data = mean_src.read(1).astype(float)\n",
    "        sd_data = sd_src.read(1).astype(float)\n",
    "        \n",
    "        profile = mean_src.profile.copy()\n",
    "        profile.update(dtype=rasterio.float32, count=1)\n",
    "        \n",
    "        valid_mask = (~np.isnan(mean_data)) & (~np.isnan(sd_data)) & (mean_data != 0)\n",
    "        \n",
    "        cv_data = np.full(mean_data.shape, np.nan, dtype=np.float32)\n",
    "        \n",
    "        # ---- KEY UPDATE: take absolute value ----\n",
    "        cv_data[valid_mask] = np.abs(sd_data[valid_mask] / mean_data[valid_mask]) * 100\n",
    "        \n",
    "        if output_path:\n",
    "            with rasterio.open(output_path, 'w', **profile) as dst:\n",
    "                dst.write(cv_data, 1)\n",
    "            print(f\"CV raster saved to: {output_path}\")\n",
    "        \n",
    "        return cv_data, profile\n",
    "\n",
    "\n",
    "def main():\n",
    "    base_path = \"E:/minmin/spreadingmapping/future\"\n",
    "    mean_file = os.path.join(base_path, \"passive_top_prediction.tif\")#replace with active_top, SOC_top, active_sub,SOC_sub,passive_sub\n",
    "    sd_file = os.path.join(base_path, \"passive_top_uncertainty_spread.tif\")#replace with active_top, SOC_top, active_sub,SOC_sub,passive_sub\n",
    "    output_file = os.path.join(base_path, \"passive_top_CV.tif\")#replace with active_top, SOC_top, active_sub,SOC_sub,passive_sub\n",
    "\n",
    "    if not os.path.exists(mean_file):\n",
    "        print(f\"Error: Mean file not found at {mean_file}\")\n",
    "        return\n",
    "    if not os.path.exists(sd_file):\n",
    "        print(f\"Error: SD file not found at {sd_file}\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nCalculating CV...\")\n",
    "    cv_data, profile = calculate_cv(mean_file, sd_file, output_file)\n",
    "    \n",
    "    valid_cv = cv_data[~np.isnan(cv_data)]\n",
    "    if len(valid_cv) > 0:\n",
    "        print(\"\\nCV Statistics:\")\n",
    "        print(f\"  Min CV: {valid_cv.min():.2f}%\")\n",
    "        print(f\"  Max CV: {valid_cv.max():.2f}%\")\n",
    "        print(f\"  Mean CV: {valid_cv.mean():.2f}%\")\n",
    "        print(f\"  Median CV: {np.median(valid_cv):.2f}%\")\n",
    "        print(f\"  Valid pixels: {len(valid_cv)}\")\n",
    "        print(f\"  Invalid pixels: {np.sum(np.isnan(cv_data) | np.isinf(cv_data))}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137ea9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Raster-Based Coefficient of Variation (CV) for lability Index (LI)\n",
    "import rasterio\n",
    "import numpy as np\n",
    "\n",
    "active_path = r\"E:/minmin/spreadingmapping/future/active_sub_future_uncertainty_spread.tif\" #alternatively replace with active_top\n",
    "passive_path = r\"E:/minmin/spreadingmapping/future/passive_sub_future_uncertainty_spread.tif\"#alternatively replace with active_top\n",
    "output_path = r\"E:/minmin/spreadingmapping/future/LI_future_sub_CV.tif\"\n",
    "\n",
    "with rasterio.open(active_path) as src_a, rasterio.open(passive_path) as src_p:\n",
    "\n",
    "    meta = src_a.meta.copy()\n",
    "    meta.update(dtype=\"float32\")\n",
    "\n",
    "    with rasterio.open(output_path, \"w\", **meta) as dst:\n",
    "\n",
    "        # 逐块处理，省内存\n",
    "        for ji, window in src_a.block_windows(1):\n",
    "\n",
    "            a = src_a.read(1, window=window)\n",
    "            p = src_p.read(1, window=window)\n",
    "\n",
    "            out = np.sqrt(a.astype(\"float64\")**2 + p.astype(\"float64\")**2)\n",
    "\n",
    "            dst.write(out.astype(\"float32\"), 1, window=window)\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_venv)",
   "language": "python",
   "name": "my_venv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
